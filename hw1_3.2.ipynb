{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n",
    "import pickle as pkl\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_train_df = pd.read_csv('data/mnli_train.tsv','\\t')\n",
    "mnli_val_df = pd.read_csv('data/mnli_val.tsv','\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = spacy.load('en_core_web_sm')\n",
    "# punctuations = string.punctuation\n",
    "\n",
    "# # lowercase and remove punctuation\n",
    "# def tokenize(sent):\n",
    "#     tokens = tokenizer(sent)\n",
    "#     return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "\n",
    "# # tokenize datasets\n",
    "# # dataset1 contains sentence1, dataset2 contains sentence2\n",
    "# def tokenize_dataset(dataset1, dataset2):\n",
    "#     token_dataset1 = []\n",
    "#     token_dataset2 = []\n",
    "#     # we are keeping track of all tokens in dataset \n",
    "#     # in order to create vocabulary later\n",
    "#     all_tokens = []\n",
    "    \n",
    "#     for sample in dataset1:\n",
    "#         tokens = tokenize(sample)\n",
    "#         token_dataset1.append(tokens)\n",
    "#         all_tokens += tokens\n",
    "        \n",
    "#     for sample in dataset2:\n",
    "#         tokens = tokenize(sample)\n",
    "#         token_dataset2.append(tokens)\n",
    "#         all_tokens += tokens\n",
    "#     return token_dataset1, token_dataset2, all_tokens\n",
    "\n",
    "\n",
    "# train_sen1_tokens = mnli_train_travel['sentence1'].tolist()\n",
    "# train_sen2_tokens = mnli_train_travel['sentence2'].tolist()\n",
    "\n",
    "# val_sen1_tokens = mnli_val_travel['sentence1'].tolist()\n",
    "# val_sen2_tokens = mnli_val_travel['sentence2'].tolist()\n",
    "\n",
    "# train_target = mnli_train_travel['label'].tolist()\n",
    "# val_target = mnli_val_travel['label'].tolist()\n",
    "\n",
    "# # train set tokens\n",
    "# print(\"Tokenizing train data separately (sentence 1 and sentence 2)\")\n",
    "# train_data_tokens_1, train_data_tokens_2, all_train_tokens = tokenize_dataset(train_sen1_tokens, train_sen2_tokens)\n",
    "# pkl.dump(train_data_tokens_1, open(\"data/mnli_train_travel_tokens_1.p\", \"wb\"))\n",
    "# pkl.dump(train_data_tokens_2, open(\"data/mnli_train_travel_tokens_2.p\", \"wb\"))\n",
    "# pkl.dump(all_train_tokens, open(\"data/mnli_train_travel_concat_tokens.p\", \"wb\"))\n",
    "\n",
    "# # val set tokens\n",
    "# print(\"Tokenizing val data\")\n",
    "# val_data_tokens_1, val_data_tokens_2, _ = tokenize_dataset(val_sen1_tokens, val_sen2_tokens)\n",
    "# pkl.dump(val_data_tokens_1, open(\"data/mnli_val_travel_tokens_1.p\", \"wb\"))\n",
    "# pkl.dump(val_data_tokens_2, open(\"data/mnli_val_travel_tokens_2.p\", \"wb\"))\n",
    "\n",
    "# print(\"finish tokenizing all data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionPyTorch(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_dim, n_out, inter):\n",
    "        \"\"\"\n",
    "        n_in: Number of features\n",
    "        n_out: Number of output classes\n",
    "        \"\"\"\n",
    "        # Initialize the parent class - this is a Python requirement\n",
    "        super().__init__()\n",
    "\n",
    "        # Set up out linear layer. This initializes the weights\n",
    "        # Note that self.linear is itself a nn.Module, nested within\n",
    "        #   this module\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.inter = inter\n",
    "        if self.inter == 'cat':\n",
    "            self.linear = nn.Linear(emb_dim*2, n_out)\n",
    "        elif self.inter == 'mul':\n",
    "            self.linear = nn.Linear(emb_dim, n_out)\n",
    "\n",
    "\n",
    "        # Explicitly initialize the weights with the initialization\n",
    "        #   scheme we want.\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, data_1, data_2, length_1, length_2):\n",
    "        \"\"\"\n",
    "        x: Input data [N, k]\n",
    "        ---\n",
    "        Returns: log probabilities of each class [N, c]\n",
    "        \"\"\"\n",
    "        # Apply the linear function to get our logit (real numbers)\n",
    "        out_1 = self.embed(data_1)\n",
    "        out_1 = torch.sum(out_1, dim=1)\n",
    "        out_1 /= length_1.view(length_1.size()[0],1).expand_as(out_1).float()\n",
    "\n",
    "        out_2 = self.embed(data_2)\n",
    "        out_2 = torch.sum(out_2, dim=1)\n",
    "        out_2 /= length_2.view(length_2.size()[0],1).expand_as(out_2).float()\n",
    "\n",
    "        if self.inter == 'cat':\n",
    "            out = torch.cat((out_1, out_2), dim=1)\n",
    "        elif self.inter == 'mul':\n",
    "            out = torch.mul(out_1, out_2)\n",
    "\n",
    "        out = self.linear(out)\n",
    "\n",
    "        # Apply log_softmax to get logs of normalized probabilities\n",
    "        return F.log_softmax(out, dim=1)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        # Use some specific initialization schemes\n",
    "        nn.init.xavier_normal_(self.linear.weight)\n",
    "        nn.init.uniform_(self.linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnli_pipeline(model_file,genre):\n",
    "    mnli_train = mnli_train_df[mnli_train_df['genre']==genre]\n",
    "    mnli_val = mnli_val_df[mnli_val_df['genre']==genre]\n",
    "    mapping = {'neutral': 0, 'entailment': 1, 'contradiction': 2}\n",
    "    mnli_train = mnli_train.applymap(lambda s: mapping.get(s) if s in mapping else s)\n",
    "    mnli_val = mnli_val.applymap(lambda s: mapping.get(s) if s in mapping else s)\n",
    "    train_target = list(mnli_train['label'])\n",
    "    val_target = list(mnli_val['label'])\n",
    "    \n",
    "    mnli_train_tokens_1 = pkl.load(open(\"data/mnli_train_{}_tokens_1.p\".format(genre), \"rb\"))\n",
    "    mnli_train_tokens_2 = pkl.load(open(\"data/mnli_train_{}_tokens_2.p\".format(genre), \"rb\"))\n",
    "    mnli_train_all_tokens = pkl.load(open(\"data/mnli_train_{}_concat_tokens.p\".format(genre), \"rb\"))\n",
    "\n",
    "    mnli_val_tokens_1 = pkl.load(open(\"data/mnli_val_{}_tokens_1.p\".format(genre), \"rb\"))\n",
    "    mnli_val_tokens_2 = pkl.load(open(\"data/mnli_val_{}_tokens_2.p\".format(genre), \"rb\"))\n",
    "    \n",
    "    max_vocab_size = 8000\n",
    "    # save index 0 for unk and 1 for pad\n",
    "    PAD_IDX = 0\n",
    "    UNK_IDX = 1\n",
    "\n",
    "    def build_vocab(all_tokens):\n",
    "        # Returns:\n",
    "        # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "        # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "        token_counter = Counter(all_tokens)\n",
    "        vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "        id2token = list(vocab) #what token is assigned to a number\n",
    "        token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "        id2token = ['<pad>', '<unk>'] + id2token\n",
    "        token2id['<pad>'] = PAD_IDX \n",
    "        token2id['<unk>'] = UNK_IDX\n",
    "        return token2id, id2token\n",
    "\n",
    "    token2id, id2token = build_vocab(mnli_train_all_tokens)\n",
    "    \n",
    "    # convert token to id in the dataset\n",
    "    def token2index_dataset(tokens_data):\n",
    "        indices_data = []\n",
    "        for tokens in tokens_data:\n",
    "            index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "            indices_data.append(index_list)\n",
    "        return indices_data\n",
    "\n",
    "    train_data_indices1 = token2index_dataset(mnli_train_tokens_1)\n",
    "    train_data_indices2 = token2index_dataset(mnli_train_tokens_2)\n",
    "\n",
    "    val_data_indices1 = token2index_dataset(mnli_val_tokens_1)\n",
    "    val_data_indices2 = token2index_dataset(mnli_val_tokens_2)\n",
    "    \n",
    "    #param to be tuned\n",
    "    MAX_SENTENCE_LENGTH = 100\n",
    "    BATCH_SIZE = 32\n",
    "    \n",
    "    class BuildDataset(Dataset):\n",
    "\n",
    "        def __init__(self, data_list1, data_list2, target_list):\n",
    "\n",
    "            self.data_list1 = data_list1\n",
    "            self.data_list2 = data_list2\n",
    "            self.target_list = target_list\n",
    "            assert (len(self.data_list1) == len(self.target_list))\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data_list1)\n",
    "\n",
    "        def __getitem__(self, key):\n",
    "            \"\"\"\n",
    "            Triggered when you call dataset[i]\n",
    "            \"\"\" \n",
    "            token_idx1 = self.data_list1[key][:MAX_SENTENCE_LENGTH]\n",
    "            token_idx2 = self.data_list2[key][:MAX_SENTENCE_LENGTH]\n",
    "            label = self.target_list[key]\n",
    "            return [token_idx1, len(token_idx1), token_idx2, len(token_idx2), label]\n",
    "        \n",
    "    \n",
    "    train_dataset = BuildDataset(train_data_indices1, train_data_indices2, train_target)\n",
    "    \n",
    "    def collate_func(batch):\n",
    "        \"\"\"\n",
    "        Customized function for DataLoader that dynamically pads the batch so that all \n",
    "        data have the same length\n",
    "        \"\"\"\n",
    "        data_list1 = []\n",
    "        data_list2 = []\n",
    "        label_list = []\n",
    "        length_list1 = []\n",
    "        length_list2 = []\n",
    "\n",
    "        for datum in batch:\n",
    "            label_list.append(datum[4])\n",
    "            length_list1.append(datum[1])\n",
    "            length_list2.append(datum[3])\n",
    "        # padding\n",
    "        for datum in batch:\n",
    "            padded_vec1 = np.pad(np.array(datum[0]), \n",
    "                                    pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), #pad with 0\n",
    "                                    mode=\"constant\", constant_values=0)\n",
    "\n",
    "            padded_vec2 = np.pad(np.array(datum[2]), \n",
    "                                    pad_width=((0,MAX_SENTENCE_LENGTH-datum[3])), #pad with 0\n",
    "                                    mode=\"constant\", constant_values=0)\n",
    "\n",
    "            data_list1.append(padded_vec1)\n",
    "            data_list2.append(padded_vec2)\n",
    "        return [torch.from_numpy(np.array(data_list1)), torch.from_numpy(np.array(data_list2)),\n",
    "                torch.LongTensor(length_list1), torch.LongTensor(length_list2),\n",
    "                torch.LongTensor(label_list)]\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=collate_func,\n",
    "                                               shuffle=True)\n",
    "\n",
    "    val_dataset = BuildDataset(val_data_indices1, val_data_indices2, val_target)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=collate_func,\n",
    "                                               shuffle=True)\n",
    "      \n",
    "    def test_model(loader, model):\n",
    "        \"\"\"\n",
    "        Help function that tests the model's performance on a dataset\n",
    "        @param: loader - data loader for the dataset to test against\n",
    "        \"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.eval()\n",
    "        for data1, data2, len1, len2, labels in loader:\n",
    "            data_batch1, data_batch2, len_batch1, len_batch2,label_batch = data1, data2, len1, len2, labels\n",
    "            outputs = F.softmax(model(data_batch1, data_batch2, len_batch1, len_batch2), dim=1)\n",
    "            predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "        return (100 * correct / total)\n",
    "\n",
    "    model = LogisticRegressionPyTorch(len(id2token), emb_dim, 3,'mul')\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    val_acc = test_model(val_loader, model)\n",
    "    return val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.25254582484725"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dim = 100\n",
    "interact = 'mul'\n",
    "model_file = 'lr_9_iter_tune.ckpt'\n",
    "genre = 'travel'\n",
    "mnli_pipeline(model_file,'travel')\n",
    "# model = LogisticRegressionPyTorch(len(id2token), emb_dim, 3,'mul')\n",
    "# model.load_state_dict(torch.load('model/lr_9_iter_tune.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fiction = mnli_pipeline('model/lr_9_iter_tune.ckpt','fiction')\n",
    "# slate = mnli_pipeline('model/lr_9_iter_tune.ckpt','slate')\n",
    "# government = mnli_pipeline('model/lr_9_iter_tune.ckpt','government')\n",
    "# telephone = mnli_pipeline('model/lr_9_iter_tune.ckpt','telephone')\n",
    "# travel = mnli_pipeline('model/lr_9_iter_tune.ckpt','travel')\n",
    "# l = [fiction,slate,government,telephone,travel]\n",
    "# l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
