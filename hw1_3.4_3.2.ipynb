{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Frozen_NN(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, drop):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(Frozen_NN, self).__init__()\n",
    "        # pay attention to padding_idx\n",
    "        l1_dim = 100\n",
    "        l2_dim = 100\n",
    "        self.embed = nn.Embedding.from_pretrained(weight_np, freeze=False, padding_idx=0)\n",
    "        \n",
    "        self.hidden1 = nn.Linear(300*2, l1_dim)\n",
    "        self.dropout = nn.Dropout(p=drop)\n",
    "        self.hidden2 = nn.Linear(l1_dim, l2_dim)\n",
    "        self.linear = nn.Linear(l2_dim, 3)\n",
    "        \n",
    "    def forward(self, data1, data2, length_1, length_2):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data1: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review in sentence1 that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param data2: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review in sentence2 that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length1: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data1.\n",
    "        @param length2: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data2.\n",
    "        \"\"\"\n",
    "        out_1 = self.embed(data1)\n",
    "        m = (data1 == 1)\n",
    "        m = m.unsqueeze(2).repeat(1, 1, 300).type(torch.FloatTensor)\n",
    "        out_1 = m * out_1 + (1-m) * out_1.clone().detach()\n",
    "        out_1 = self.dropout(out_1)\n",
    "        out_1 = torch.sum(out_1, dim=1)\n",
    "        out_1 /= length_1.view(length_1.size()[0],1).expand_as(out_1).float()\n",
    "\n",
    "        \n",
    "        out_2 = self.embed(data2)\n",
    "        m = (data2 == 1)\n",
    "        m = m.unsqueeze(2).repeat(1, 1, 300).type(torch.FloatTensor)\n",
    "        out_2 = m * out_2 + (1-m) * out_2.clone().detach()\n",
    "        out_2 = self.dropout(out_2)\n",
    "        out_2 = torch.sum(out_2, dim=1)\n",
    "        out_2 /= length_2.view(length_2.size()[0],1).expand_as(out_2).float()\n",
    "        \n",
    "        out = torch.cat((out_1, out_2), dim=1)\n",
    "        \n",
    "        out = F.relu(self.hidden1(out.float()))\n",
    "        out = F.relu(self.hidden2(out.float()))\n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnli_pipeline(model_file,genre):\n",
    "    mnli_train_df = pd.read_csv(\"./data/mnli_train.tsv\",sep='\\t')\n",
    "    mnli_val_df = pd.read_csv(\"./data/mnli_val.tsv\",sep='\\t')\n",
    "    mnli_train = mnli_train_df[mnli_train_df['genre']==genre]\n",
    "    mnli_val = mnli_val_df[mnli_val_df['genre']==genre]\n",
    "    mapping = {'neutral': 0, 'entailment': 1, 'contradiction': 2}\n",
    "    mnli_train = mnli_train.applymap(lambda s: mapping.get(s) if s in mapping else s)\n",
    "    mnli_val = mnli_val.applymap(lambda s: mapping.get(s) if s in mapping else s)\n",
    "    train_target = list(mnli_train['label'])\n",
    "    val_target = list(mnli_val['label'])\n",
    "    \n",
    "    mnli_train_tokens_1 = pkl.load(open(\"data/mnli_train_{}_tokens_1.p\".format(genre), \"rb\"))\n",
    "    mnli_train_tokens_2 = pkl.load(open(\"data/mnli_train_{}_tokens_2.p\".format(genre), \"rb\"))\n",
    "    mnli_train_all_tokens = pkl.load(open(\"data/mnli_train_{}_concat_tokens.p\".format(genre), \"rb\"))\n",
    "\n",
    "    mnli_val_tokens_1 = pkl.load(open(\"data/mnli_val_{}_tokens_1.p\".format(genre), \"rb\"))\n",
    "    mnli_val_tokens_2 = pkl.load(open(\"data/mnli_val_{}_tokens_2.p\".format(genre), \"rb\"))\n",
    "    \n",
    "    max_vocab_size = 10000\n",
    "    # save index 0 for unk and 1 for pad\n",
    "    PAD_IDX = 0\n",
    "    UNK_IDX = 1\n",
    "\n",
    "    def build_vocab(all_tokens):\n",
    "        # Returns:\n",
    "        # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "        # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "        token_counter = Counter(all_tokens)\n",
    "        vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "        id2token = list(vocab) #what token is assigned to a number\n",
    "        token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "        id2token = ['<pad>', '<unk>'] + id2token\n",
    "        token2id['<pad>'] = PAD_IDX \n",
    "        token2id['<unk>'] = UNK_IDX\n",
    "        return token2id, id2token\n",
    "\n",
    "    token2id, id2token = build_vocab(mnli_train_all_tokens)\n",
    "    \n",
    "    # convert token to id in the dataset\n",
    "    def token2index_dataset(tokens_data):\n",
    "        indices_data = []\n",
    "        for tokens in tokens_data:\n",
    "            index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "            indices_data.append(index_list)\n",
    "        return indices_data\n",
    "\n",
    "    train_data_indices1 = token2index_dataset(mnli_train_tokens_1)\n",
    "    train_data_indices2 = token2index_dataset(mnli_train_tokens_2)\n",
    "\n",
    "    val_data_indices1 = token2index_dataset(mnli_val_tokens_1)\n",
    "    val_data_indices2 = token2index_dataset(mnli_val_tokens_2)\n",
    "    \n",
    "    #param to be tuned\n",
    "    MAX_SENTENCE_LENGTH = 100\n",
    "    BATCH_SIZE = 32\n",
    "    \n",
    "    class BuildDataset(Dataset):\n",
    "\n",
    "        def __init__(self, data_list1, data_list2, target_list):\n",
    "\n",
    "            self.data_list1 = data_list1\n",
    "            self.data_list2 = data_list2\n",
    "            self.target_list = target_list\n",
    "            assert (len(self.data_list1) == len(self.target_list))\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data_list1)\n",
    "\n",
    "        def __getitem__(self, key):\n",
    "            \"\"\"\n",
    "            Triggered when you call dataset[i]\n",
    "            \"\"\" \n",
    "            token_idx1 = self.data_list1[key][:MAX_SENTENCE_LENGTH]\n",
    "            token_idx2 = self.data_list2[key][:MAX_SENTENCE_LENGTH]\n",
    "            label = self.target_list[key]\n",
    "            return [token_idx1, len(token_idx1), token_idx2, len(token_idx2), label]\n",
    "        \n",
    "    \n",
    "    train_dataset = BuildDataset(train_data_indices1, train_data_indices2, train_target)\n",
    "    \n",
    "    def collate_func(batch):\n",
    "        \"\"\"\n",
    "        Customized function for DataLoader that dynamically pads the batch so that all \n",
    "        data have the same length\n",
    "        \"\"\"\n",
    "        data_list1 = []\n",
    "        data_list2 = []\n",
    "        label_list = []\n",
    "        length_list1 = []\n",
    "        length_list2 = []\n",
    "\n",
    "        for datum in batch:\n",
    "            label_list.append(datum[4])\n",
    "            length_list1.append(datum[1])\n",
    "            length_list2.append(datum[3])\n",
    "        # padding\n",
    "        for datum in batch:\n",
    "            padded_vec1 = np.pad(np.array(datum[0]), \n",
    "                                    pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), #pad with 0\n",
    "                                    mode=\"constant\", constant_values=0)\n",
    "\n",
    "            padded_vec2 = np.pad(np.array(datum[2]), \n",
    "                                    pad_width=((0,MAX_SENTENCE_LENGTH-datum[3])), #pad with 0\n",
    "                                    mode=\"constant\", constant_values=0)\n",
    "\n",
    "            data_list1.append(padded_vec1)\n",
    "            data_list2.append(padded_vec2)\n",
    "        return [torch.from_numpy(np.array(data_list1)), torch.from_numpy(np.array(data_list2)),\n",
    "                torch.LongTensor(length_list1), torch.LongTensor(length_list2),\n",
    "                torch.LongTensor(label_list)]\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=collate_func,\n",
    "                                               shuffle=True)\n",
    "\n",
    "    val_dataset = BuildDataset(val_data_indices1, val_data_indices2, val_target)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=collate_func,\n",
    "                                               shuffle=True)\n",
    "      \n",
    "    def test_model(loader, model):\n",
    "        \"\"\"\n",
    "        Help function that tests the model's performance on a dataset\n",
    "        @param: loader - data loader for the dataset to test against\n",
    "        \"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.eval()\n",
    "        for data1, data2, len1, len2, labels in loader:\n",
    "            data_batch1, data_batch2, len_batch1, len_batch2,label_batch = data1, data2, len1, len2, labels\n",
    "            outputs = F.softmax(model(data_batch1, data_batch2, len_batch1, len_batch2), dim=1)\n",
    "            predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "        return (100 * correct / total)\n",
    "\n",
    "    model = pkl.load(open(model_file, \"rb\"))\n",
    "    val_acc = test_model(val_loader, model)\n",
    "    return genre,val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('travel', 31.16089613034623)\n",
      "('fiction', 34.57286432160804)\n",
      "('government', 31.594488188976378)\n",
      "('slate', 33.63273453093812)\n",
      "('telephone', 32.33830845771144)\n"
     ]
    }
   ],
   "source": [
    "model = \"finalized_frozen_embed_model.sav\"\n",
    "genre_list = ['travel','fiction', 'government', 'slate', 'telephone']\n",
    "for i in genre_list:\n",
    "    print(mnli_pipeline(model,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
